{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Retinal Vessel Segmentation Model (with MPI parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ColossalAI deepspeed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import colossalai\n",
    "from colossalai.core import global_context as gpc\n",
    "from colossalai.logging import get_dist_logger\n",
    "import argparse\n",
    "from colossalai.trainer import Trainer\n",
    "\n",
    "from dataloaders.vessel import RetinalVesselSegmentation\n",
    "from dataloaders import custom_transforms as tr\n",
    "from networks.unet.unet_model import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training parameters\n",
    "Using fundus image datasets from four domains. Dataset available upon request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n"
     ]
    }
   ],
   "source": [
    "datasetTrain = [0,1,2]\n",
    "datasetTest = [3]\n",
    "data_dir = '/work/zhangyq/RVS' #Dataset directory\n",
    "config = './configs/Fundus/FundusConfigBase.py'\n",
    "gpc.load_config(config)\n",
    "colossalai.context.config.Config.from_file(config)\n",
    "splitidTrain = []\n",
    "for x in datasetTrain:\n",
    "    splitidTrain.append(int(x))\n",
    "splitidTest = []\n",
    "for x in datasetTest:\n",
    "    splitidTest.append(int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize distributed environment (Supporting MPI/Slurm/torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:35:14,151 INFO: Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:35:14,153 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:35:14,154 INFO: Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:35:14,155 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:35:14,157 INFO: Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:35:14,158 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "colossalai - root - 2021-12-11 03:35:14,166 INFO: process rank 0 is bound to device 0\n",
      "colossalai - root - 2021-12-11 03:35:14,171 INFO: initialized seed on rank 0, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.\n",
      "colossalai - root - 2021-12-11 03:35:14,172 INFO: Distributed environment is initialized, data parallel size: 1, pipeline parallel size: 1, tensor parallel size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: variables which starts with __, is a module or class declaration are omitted\n"
     ]
    }
   ],
   "source": [
    "if 'OMPI_COMM_WORLD_RANK' in os.environ:\n",
    "    colossalai.launch_from_openmpi(config=config,\n",
    "    host='gpu01',\n",
    "    port='11455',\n",
    "    backend='nccl')\n",
    "elif 'SLURM_PROCID' in os.environ:\n",
    "    colossalai.launch_from_slurm(config=config,\n",
    "    host='localhost',\n",
    "    port='11455',\n",
    "    backend='nccl')\n",
    "elif 'WORLD_SIZE' in os.environ:\n",
    "    colossalai.launch_from_torch(config=config,\n",
    "    host='localhost',\n",
    "    port='11455',\n",
    "    backend='nccl')\n",
    "else:\n",
    "    colossalai.launch(\n",
    "        config=config,\n",
    "        host='localhost',\n",
    "        port='11455',\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        backend='nccl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading train data from: /work/zhangyq/RVS/CHASEDB1/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/DRIVE/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/HRF/train\n",
      "img_num: 55\n",
      "key STARE has no data\n",
      "20 images in CHASEDB1\n",
      "20 images in DRIVE\n",
      "15 images in HRF\n",
      "-----Total number of images in train: 55\n",
      "==> Loading test data from: /work/zhangyq/RVS/STARE\n",
      "img_num: 10\n",
      "key CHASEDB1 has no data\n",
      "key DRIVE has no data\n",
      "key HRF has no data\n",
      "10 images in STARE\n",
      "-----Total number of images in test: 10\n"
     ]
    }
   ],
   "source": [
    "composed_transforms_tr = transforms.Compose([\n",
    "    tr.RandomScaleCrop(256),\n",
    "    # tr.RandomCrop(512),\n",
    "    # tr.RandomRotate(),\n",
    "    # tr.RandomFlip(),\n",
    "    # tr.elastic_transform(),\n",
    "    # tr.add_salt_pepper_noise(),\n",
    "    # tr.adjust_light(),\n",
    "    # tr.eraser(),\n",
    "    tr.Normalize_tf(),\n",
    "    tr.ToTensor()\n",
    "])\n",
    "\n",
    "composed_transforms_ts = transforms.Compose([\n",
    "    tr.RandomCrop(256),\n",
    "    tr.Normalize_tf(),\n",
    "    tr.ToTensor()\n",
    "])\n",
    "\n",
    "domain = RetinalVesselSegmentation(base_dir=data_dir, phase='train', splitid=splitidTrain,\n",
    "                                                        transform=composed_transforms_tr)\n",
    "train_loader = DataLoader(domain, batch_size=8, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "domain_val = RetinalVesselSegmentation(base_dir=data_dir, phase='test', splitid=splitidTest,\n",
    "                                    transform=composed_transforms_ts)\n",
    "val_loader = DataLoader(domain_val, batch_size=8, shuffle=False, num_workers=1, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3,2).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Engine and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "colossalai - root - 2021-12-11 03:36:09,685 INFO: \n",
      "========== Your Config ========\n",
      "{'hooks': [{'type': 'LogMetricByEpochHook'},\n",
      "           {'type': 'LogTimingByEpochHook'},\n",
      "           {'type': 'LogMemoryByEpochHook'},\n",
      "           {'type': 'Accuracy2DHook'},\n",
      "           {'type': 'LossHook'}],\n",
      " 'logging': {'root_path': './logs'},\n",
      " 'num_epochs': 60,\n",
      " 'optimizer': {'lr': 0.001, 'type': 'Adam', 'weight_decay': 0},\n",
      " 'parallel': {'data': {'size': 1},\n",
      "              'pipeline': {'size': 1},\n",
      "              'tensor': {'mode': None, 'size': 1}},\n",
      " 'schedule': {'num_microbatches': 8}}\n",
      "================================\n",
      "\n",
      "colossalai - root - 2021-12-11 03:36:09,686 INFO: cuDNN benchmark = True, deterministic = False\n",
      "colossalai - root - 2021-12-11 03:36:09,690 WARNING: No PyTorch DDP or gradient handler is set up, please make sure you do not need to all-reduce the gradients after a training step.\n",
      "colossalai - root - 2021-12-11 03:36:09,691 INFO: engine is built\n",
      "colossalai - root - 2021-12-11 03:36:09,692 INFO: trainer is built\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.99)\n",
    ")\n",
    "def batch_data_process_func(sample):\n",
    "    image = None\n",
    "    label = None\n",
    "    for domain in sample:\n",
    "        if image is None:\n",
    "            image = domain['image']\n",
    "            label = domain['label']\n",
    "        else:\n",
    "            image = torch.cat([image, domain['image']], 0)\n",
    "            label = torch.cat([label, domain['label']], 0)\n",
    "    return image,label\n",
    "logger = get_dist_logger('root')\n",
    "schedule=colossalai.engine.schedule.NonPipelineSchedule()\n",
    "# lr_scheduler=colossalai.nn.lr_scheduler.CosineAnnealingLR(optim, 1000)\n",
    "criterion=torch.nn.BCELoss()\n",
    "schedule.batch_data_process_func = batch_data_process_func\n",
    "engine, train_dataloader, test_dataloader, lr_scheduler = colossalai.initialize(model=model,\n",
    "optimizer=optim,\n",
    "criterion=criterion,\n",
    "train_dataloader=train_loader,\n",
    "test_dataloader=val_loader,\n",
    "verbose=True,)\n",
    "\n",
    "logger.info(\"engine is built\", ranks=[0])\n",
    "\n",
    "trainer = Trainer(engine=engine,\n",
    "        schedule=schedule, logger=logger)\n",
    "logger.info(\"trainer is built\", ranks=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "colossalai - root - 2021-12-11 03:36:10,397 INFO: start training\n",
      "colossalai - root - 2021-12-11 03:36:10,398 INFO: Lower value means higher priority for calling hook function\n",
      "[Epoch 0 train]: 100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n",
      "[Epoch 0 val]: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\n",
      "[Epoch 1 train]: 100%|██████████| 8/8 [00:01<00:00,  5.09it/s]\n",
      "[Epoch 2 train]: 100%|██████████| 8/8 [00:01<00:00,  5.05it/s]\n",
      "[Epoch 2 val]: 100%|██████████| 2/2 [00:00<00:00, 15.81it/s]\n",
      "[Epoch 3 train]: 100%|██████████| 8/8 [00:01<00:00,  5.19it/s]\n",
      "[Epoch 4 train]: 100%|██████████| 8/8 [00:01<00:00,  5.33it/s]\n",
      "[Epoch 4 val]: 100%|██████████| 2/2 [00:00<00:00, 15.30it/s]\n",
      "[Epoch 5 train]: 100%|██████████| 8/8 [00:01<00:00,  5.14it/s]\n",
      "[Epoch 6 train]: 100%|██████████| 8/8 [00:01<00:00,  5.35it/s]\n",
      "[Epoch 6 val]: 100%|██████████| 2/2 [00:00<00:00, 15.73it/s]\n",
      "[Epoch 7 train]: 100%|██████████| 8/8 [00:01<00:00,  5.34it/s]\n",
      "[Epoch 8 train]: 100%|██████████| 8/8 [00:01<00:00,  5.44it/s]\n",
      "[Epoch 8 val]: 100%|██████████| 2/2 [00:00<00:00, 16.00it/s]\n",
      "[Epoch 9 train]: 100%|██████████| 8/8 [00:01<00:00,  5.36it/s]\n",
      "[Epoch 10 train]: 100%|██████████| 8/8 [00:01<00:00,  5.34it/s]\n",
      "[Epoch 10 val]: 100%|██████████| 2/2 [00:00<00:00, 14.87it/s]\n",
      "[Epoch 11 train]: 100%|██████████| 8/8 [00:01<00:00,  5.11it/s]\n",
      "[Epoch 12 train]: 100%|██████████| 8/8 [00:01<00:00,  5.36it/s]\n",
      "[Epoch 12 val]: 100%|██████████| 2/2 [00:00<00:00, 15.20it/s]\n",
      "[Epoch 13 train]: 100%|██████████| 8/8 [00:01<00:00,  5.44it/s]\n",
      "[Epoch 14 train]: 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n",
      "[Epoch 14 val]: 100%|██████████| 2/2 [00:00<00:00, 15.20it/s]\n",
      "[Epoch 15 train]: 100%|██████████| 8/8 [00:01<00:00,  5.15it/s]\n",
      "[Epoch 16 train]: 100%|██████████| 8/8 [00:01<00:00,  5.26it/s]\n",
      "[Epoch 16 val]: 100%|██████████| 2/2 [00:00<00:00, 15.82it/s]\n",
      "[Epoch 17 train]: 100%|██████████| 8/8 [00:01<00:00,  5.32it/s]\n",
      "[Epoch 18 train]: 100%|██████████| 8/8 [00:01<00:00,  5.39it/s]\n",
      "[Epoch 18 val]: 100%|██████████| 2/2 [00:00<00:00, 15.11it/s]\n",
      "[Epoch 19 train]: 100%|██████████| 8/8 [00:01<00:00,  5.06it/s]\n",
      "[Epoch 20 train]: 100%|██████████| 8/8 [00:01<00:00,  5.22it/s]\n",
      "[Epoch 20 val]: 100%|██████████| 2/2 [00:00<00:00, 15.41it/s]\n",
      "[Epoch 21 train]: 100%|██████████| 8/8 [00:01<00:00,  5.13it/s]\n",
      "[Epoch 22 train]: 100%|██████████| 8/8 [00:01<00:00,  5.03it/s]\n",
      "[Epoch 22 val]: 100%|██████████| 2/2 [00:00<00:00, 16.09it/s]\n",
      "[Epoch 23 train]: 100%|██████████| 8/8 [00:01<00:00,  5.42it/s]\n",
      "[Epoch 24 train]: 100%|██████████| 8/8 [00:01<00:00,  5.34it/s]\n",
      "[Epoch 24 val]: 100%|██████████| 2/2 [00:00<00:00, 15.08it/s]\n",
      "[Epoch 25 train]: 100%|██████████| 8/8 [00:01<00:00,  5.12it/s]\n",
      "[Epoch 26 train]: 100%|██████████| 8/8 [00:01<00:00,  5.39it/s]\n",
      "[Epoch 26 val]: 100%|██████████| 2/2 [00:00<00:00, 16.39it/s]\n",
      "[Epoch 27 train]: 100%|██████████| 8/8 [00:01<00:00,  5.16it/s]\n",
      "[Epoch 28 train]: 100%|██████████| 8/8 [00:01<00:00,  5.38it/s]\n",
      "[Epoch 28 val]: 100%|██████████| 2/2 [00:00<00:00, 15.00it/s]\n",
      "[Epoch 29 train]: 100%|██████████| 8/8 [00:01<00:00,  5.33it/s]\n",
      "[Epoch 30 train]: 100%|██████████| 8/8 [00:01<00:00,  5.35it/s]\n",
      "[Epoch 30 val]: 100%|██████████| 2/2 [00:00<00:00, 19.09it/s]\n",
      "[Epoch 31 train]: 100%|██████████| 8/8 [00:01<00:00,  5.30it/s]\n",
      "[Epoch 32 train]: 100%|██████████| 8/8 [00:01<00:00,  5.41it/s]\n",
      "[Epoch 32 val]: 100%|██████████| 2/2 [00:00<00:00, 15.70it/s]\n",
      "[Epoch 33 train]: 100%|██████████| 8/8 [00:01<00:00,  5.49it/s]\n",
      "[Epoch 34 train]: 100%|██████████| 8/8 [00:01<00:00,  5.41it/s]\n",
      "[Epoch 34 val]: 100%|██████████| 2/2 [00:00<00:00, 15.83it/s]\n",
      "[Epoch 35 train]: 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]\n",
      "[Epoch 36 train]: 100%|██████████| 8/8 [00:01<00:00,  4.93it/s]\n",
      "[Epoch 36 val]: 100%|██████████| 2/2 [00:00<00:00, 16.12it/s]\n",
      "[Epoch 37 train]: 100%|██████████| 8/8 [00:01<00:00,  5.42it/s]\n",
      "[Epoch 38 train]: 100%|██████████| 8/8 [00:01<00:00,  4.96it/s]\n",
      "[Epoch 38 val]: 100%|██████████| 2/2 [00:00<00:00, 17.92it/s]\n",
      "[Epoch 39 train]: 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n",
      "[Epoch 40 train]: 100%|██████████| 8/8 [00:01<00:00,  5.30it/s]\n",
      "[Epoch 40 val]: 100%|██████████| 2/2 [00:00<00:00, 15.64it/s]\n",
      "[Epoch 41 train]: 100%|██████████| 8/8 [00:01<00:00,  5.40it/s]\n",
      "[Epoch 42 train]: 100%|██████████| 8/8 [00:01<00:00,  5.37it/s]\n",
      "[Epoch 42 val]: 100%|██████████| 2/2 [00:00<00:00, 14.82it/s]\n",
      "[Epoch 43 train]: 100%|██████████| 8/8 [00:01<00:00,  5.27it/s]\n",
      "[Epoch 44 train]: 100%|██████████| 8/8 [00:01<00:00,  5.08it/s]\n",
      "[Epoch 44 val]: 100%|██████████| 2/2 [00:00<00:00, 15.24it/s]\n",
      "[Epoch 45 train]: 100%|██████████| 8/8 [00:01<00:00,  5.32it/s]\n",
      "[Epoch 46 train]: 100%|██████████| 8/8 [00:01<00:00,  4.95it/s]\n",
      "[Epoch 46 val]: 100%|██████████| 2/2 [00:00<00:00, 14.91it/s]\n",
      "[Epoch 47 train]: 100%|██████████| 8/8 [00:01<00:00,  5.21it/s]\n",
      "[Epoch 48 train]: 100%|██████████| 8/8 [00:01<00:00,  5.34it/s]\n",
      "[Epoch 48 val]: 100%|██████████| 2/2 [00:00<00:00, 15.35it/s]\n",
      "[Epoch 49 train]: 100%|██████████| 8/8 [00:01<00:00,  5.40it/s]\n",
      "[Epoch 50 train]: 100%|██████████| 8/8 [00:01<00:00,  5.34it/s]\n",
      "[Epoch 50 val]: 100%|██████████| 2/2 [00:00<00:00, 15.86it/s]\n",
      "[Epoch 51 train]: 100%|██████████| 8/8 [00:01<00:00,  5.02it/s]\n",
      "[Epoch 52 train]: 100%|██████████| 8/8 [00:01<00:00,  5.31it/s]\n",
      "[Epoch 52 val]: 100%|██████████| 2/2 [00:00<00:00, 15.72it/s]\n",
      "[Epoch 53 train]: 100%|██████████| 8/8 [00:01<00:00,  5.30it/s]\n",
      "[Epoch 54 train]: 100%|██████████| 8/8 [00:01<00:00,  5.08it/s]\n",
      "[Epoch 54 val]: 100%|██████████| 2/2 [00:00<00:00, 15.40it/s]\n",
      "[Epoch 55 train]: 100%|██████████| 8/8 [00:01<00:00,  5.28it/s]\n",
      "[Epoch 56 train]: 100%|██████████| 8/8 [00:01<00:00,  5.31it/s]\n",
      "[Epoch 56 val]: 100%|██████████| 2/2 [00:00<00:00, 15.62it/s]\n",
      "[Epoch 57 train]: 100%|██████████| 8/8 [00:01<00:00,  5.34it/s]\n",
      "[Epoch 58 train]: 100%|██████████| 8/8 [00:01<00:00,  5.33it/s]\n",
      "[Epoch 58 val]: 100%|██████████| 2/2 [00:00<00:00, 14.01it/s]\n",
      "[Epoch 59 train]: 100%|██████████| 8/8 [00:01<00:00,  5.10it/s]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"start training\", ranks=[0])\n",
    "trainer.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    epochs=gpc.config.num_epochs,\n",
    "    display_progress=True,\n",
    "    test_interval=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special note\n",
    "Parallel runs can be acheived via command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:32,461 INFO: Added key: store_based_barrier_key:1 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,376 INFO: Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,449 INFO: Added key: store_based_barrier_key:1 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,455 INFO: Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,456 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,456 INFO: Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,458 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,459 INFO: Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,459 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,460 INFO: Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,464 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,465 INFO: Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,465 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,465 INFO: Added key: store_based_barrier_key:3 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,466 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,466 INFO: Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,469 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,469 INFO: Added key: store_based_barrier_key:3 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,470 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,470 INFO: Added key: store_based_barrier_key:3 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,470 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,471 INFO: Added key: store_based_barrier_key:4 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,476 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,476 INFO: Added key: store_based_barrier_key:4 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,477 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,477 INFO: Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,480 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,480 INFO: Added key: store_based_barrier_key:4 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,480 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,480 INFO: Added key: store_based_barrier_key:5 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,481 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,481 INFO: Added key: store_based_barrier_key:5 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,486 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,486 INFO: Added key: store_based_barrier_key:5 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,487 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,487 INFO: Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,487 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,487 INFO: Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,490 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,491 INFO: Added key: store_based_barrier_key:6 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,491 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,492 INFO: Added key: store_based_barrier_key:6 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,497 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,497 INFO: Added key: store_based_barrier_key:6 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,497 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,497 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,498 INFO: Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,498 INFO: Added key: store_based_barrier_key:7 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,501 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,502 INFO: Added key: store_based_barrier_key:7 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,502 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,502 INFO: Added key: store_based_barrier_key:7 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,503 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,503 INFO: Added key: store_based_barrier_key:8 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,508 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,508 INFO: Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,509 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,509 INFO: Added key: store_based_barrier_key:8 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,512 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,512 INFO: Added key: store_based_barrier_key:8 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,513 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,513 INFO: Added key: store_based_barrier_key:9 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,513 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,513 INFO: Added key: store_based_barrier_key:9 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,518 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,519 INFO: Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,519 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,519 INFO: Added key: store_based_barrier_key:9 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,520 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,520 INFO: Added key: store_based_barrier_key:10 to store for rank: 3\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,523 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,523 INFO: Added key: store_based_barrier_key:10 to store for rank: 1\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,524 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,529 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,529 INFO: Added key: store_based_barrier_key:10 to store for rank: 0\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,529 INFO: Added key: store_based_barrier_key:10 to store for rank: 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,529 INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,530 INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 4 nodes.\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,534 INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 4 nodes.\n",
      "colossalai - root - 2021-12-11 03:42:33,539 INFO: process rank 2 is bound to device 2\n",
      "colossalai - torch.distributed.distributed_c10d - 2021-12-11 03:42:33,539 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 4 nodes.\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "==> Loading train data from: /work/zhangyq/RVS/CHASEDB1/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/DRIVE/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/HRF/train\n",
      "colossalai - root - 2021-12-11 03:42:33,552 INFO: process rank 1 is bound to device 1\n",
      "colossalai - root - 2021-12-11 03:42:33,552 INFO: process rank 3 is bound to device 3\n",
      "colossalai - root - 2021-12-11 03:42:33,553 INFO: process rank 0 is bound to device 0\n",
      "colossalai - root - 2021-12-11 03:42:33,555 INFO: initialized seed on rank 0, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.\n",
      "colossalai - root - 2021-12-11 03:42:33,555 INFO: Distributed environment is initialized, data parallel size: 1, pipeline parallel size: 1, tensor parallel size: 4\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "==> Loading train data from: /work/zhangyq/RVS/CHASEDB1/train\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "warning: variables which starts with __, is a module or class declaration are omitted\n",
      "==> Loading train data from: /work/zhangyq/RVS/CHASEDB1/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/DRIVE/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/CHASEDB1/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/HRF/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/DRIVE/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/DRIVE/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/HRF/train\n",
      "==> Loading train data from: /work/zhangyq/RVS/HRF/train\n",
      "img_num: 55\n",
      "key STARE has no data\n",
      "20 images in CHASEDB1\n",
      "20 images in DRIVE\n",
      "15 images in HRF\n",
      "-----Total number of images in train: 55\n",
      "==> Loading test data from: /work/zhangyq/RVS/STARE\n",
      "img_num: 55\n",
      "key STARE has no data\n",
      "20 images in CHASEDB1\n",
      "20 images in DRIVE\n",
      "15 images in HRF\n",
      "-----Total number of images in train: 55\n",
      "==> Loading test data from: /work/zhangyq/RVS/STARE\n",
      "img_num: 55\n",
      "key STARE has no data\n",
      "20 images in CHASEDB1\n",
      "20 images in DRIVE\n",
      "15 images in HRF\n",
      "-----Total number of images in train: 55\n",
      "==> Loading test data from: /work/zhangyq/RVS/STARE\n",
      "img_num: 55\n",
      "key STARE has no data\n",
      "20 images in CHASEDB1\n",
      "20 images in DRIVE\n",
      "15 images in HRF\n",
      "-----Total number of images in train: 55\n",
      "==> Loading test data from: /work/zhangyq/RVS/STARE\n",
      "img_num: 10\n",
      "key CHASEDB1 has no data\n",
      "key DRIVE has no data\n",
      "key HRF has no data\n",
      "10 images in STARE\n",
      "-----Total number of images in test: 10\n",
      "img_num: 10\n",
      "key CHASEDB1 has no data\n",
      "key DRIVE has no data\n",
      "key HRF has no data\n",
      "10 images in STARE\n",
      "-----Total number of images in test: 10\n",
      "img_num: 10\n",
      "key CHASEDB1 has no data\n",
      "key DRIVE has no data\n",
      "key HRF has no data\n",
      "10 images in STARE\n",
      "-----Total number of images in test: 10\n",
      "img_num: 10\n",
      "key CHASEDB1 has no data\n",
      "key DRIVE has no data\n",
      "key HRF has no data\n",
      "10 images in STARE\n",
      "-----Total number of images in test: 10\n",
      "parameter numer: 17267458\n",
      "parameter numer: 17267458\n",
      "parameter numer: 17267458\n",
      "colossalai - root - 2021-12-11 03:42:43,649 INFO: \n",
      "========== Your Config ========\n",
      "{'hooks': [{'type': 'LogMetricByEpochHook'},\n",
      "           {'type': 'LogTimingByEpochHook'},\n",
      "           {'type': 'LogMemoryByEpochHook'},\n",
      "           {'type': 'Accuracy2DHook'},\n",
      "           {'type': 'LossHook'}],\n",
      " 'logging': {'root_path': './logs'},\n",
      " 'num_epochs': 60,\n",
      " 'optimizer': {'lr': 0.001, 'type': 'Adam', 'weight_decay': 0},\n",
      " 'parallel': {'data': {'size': 1},\n",
      "              'pipeline': {'size': 1},\n",
      "              'tensor': {'mode': '2d', 'size': 4}},\n",
      " 'schedule': {'num_microbatches': 8}}\n",
      "================================\n",
      "\n",
      "colossalai - root - 2021-12-11 03:42:43,649 INFO: cuDNN benchmark = True, deterministic = False\n",
      "colossalai - root - 2021-12-11 03:42:43,650 WARNING: No PyTorch DDP or gradient handler is set up, please make sure you do not need to all-reduce the gradients after a training step.\n",
      "colossalai - root - 2021-12-11 03:42:43,650 INFO: engine is built\n",
      "colossalai - root - 2021-12-11 03:42:43,650 INFO: trainer is built\n",
      "colossalai - root - 2021-12-11 03:42:43,650 INFO: start training\n",
      "colossalai - root - 2021-12-11 03:42:43,650 INFO: Lower value means higher priority for calling hook function\n",
      "parameter numer: 17267458\n",
      "[Epoch 0 train]: 100%|██████████| 8/8 [00:11<00:00,  1.43s/it]\n",
      "[Epoch 0 val]: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]\n",
      "[Epoch 1 train]:  50%|█████     | 4/8 [00:00<00:00,  4.53it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!time mpirun -np 4 python train.py --config configs/Fundus/FundusConfig1d.py --datasetTrain 012 --datasetTest 3 --data-dir ~/RVS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to achieve best performance, workarounds are still required to tweak the model for ColossalAI capabilities."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "319df3a99815ee37fcb677d533a3469e2016f5e5761bcd44f4c3ad1f1a1374ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('autoaug': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
