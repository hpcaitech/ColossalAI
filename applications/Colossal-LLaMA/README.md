<div align="center">
<h1>
Colossal-LLaMA
</h1>

 <h3>
 <a href="https://cloud.luchentech.com/">GPU Cloud Playground </a> </a> |
 <a href="https://cloud.luchentech.com/doc/docs/image/llama"> LLaMA3 Image </a>
 </h3>

</div>

## Table of Contents
- [Table of Contents](#table-of-contents)
- [News](#news)
- [Colossal-LLaMA-2-7B](#colossal-llama-2-7b)
- [Colossal-LLaMA-2-13B](#colossal-llama-2-13b)
  - [Performance Evaluation](#performance-evaluation)
    - [Model with ~7 Billion Parameters](#model-with-7-billion-parameters)
    - [Model with ~13 Billion Parameters](#model-with-13-billion-parameters)
  - [Examples](#examples)
  - [Training Logs](#training-logs)
    - [Colossal-LLaMA-2-7b-base](#colossal-llama-2-7b-base)
    - [Colossal-LLaMA-2-13b-base](#colossal-llama-2-13b-base)
  - [Inference](#inference)
    - [Import from HuggingFace](#import-from-huggingface)
    - [Import from Modelscope](#import-from-modelscope)
    - [Quick Start](#quick-start)
- [Usage](#usage)
  - [Install](#install)
    - [0. Pre-requisite](#0-pre-requisite)
    - [1. Install required packages](#1-install-required-packages)
    - [2. Install Apex](#2-install-apex)
  - [How to run](#how-to-run)
    - [1. Init Tokenizer Preparation](#1-init-tokenizer-preparation)
    - [2. Init Model Preparation](#2-init-model-preparation)
    - [3. Data Preparation](#3-data-preparation)
      - [3.1 Data for Pretraining](#31-data-for-pretraining)
      - [3.2 Data for Supervised Fine-tuning](#32-data-for-supervised-fine-tuning)
    - [4. Command Line Arguments for Training](#4-command-line-arguments-for-training)
      - [4.1 Arguments for Pretraining](#41-arguments-for-pretraining)
      - [4.2 Arguments for Supervised Fine-tuning](#42-arguments-for-supervised-fine-tuning)
    - [5. Running Command](#5-running-command)
      - [5.1 Command for Pretraining](#51-command-for-pretraining)
      - [5.2 Command for Supervised Fine-tuning](#52-command-for-supervised-fine-tuning)
- [Technical Insights](#technical-insights)
  - [Data](#data)
  - [Tokenizer](#tokenizer)
  - [Training Strategy](#training-strategy)
    - [Multi-stage Training](#multi-stage-training)
    - [Bucket-based Training](#bucket-based-training)
  - [Bridging Any Domain-specific Large Models](#bridging-any-domain-specific-large-models)
- [Citations](#citations)

## News
* [2024/4] Support continual pre-training and supervised fine-tuning of LLaMA-3.
* [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b).
[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)
[[blog]](https://hpc-ai.com/blog/colossal-llama-2-13b)
[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-13b-base)
[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-13b-base/summary)
* [2023/09] [One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific Llm Solution](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution).
[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)
[[blog]](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)
[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base)
[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-7b-base/summary)

## Colossal-LLaMA-2-7B
The [Colossal-AI](https://github.com/hpcaitech/ColossalAI) team has introduced the open-source model **Colossal-LLaMA-2-7B-base**. This model, a derivation of LLaMA-2, has undergone continual pre-training involving approximately 8.5 billion tokens over a duration of 15 hours with 64 A800 GPUs. At a cost of **less than $1,000**, you can achieve results **similar to those that cost millions of dollars to pretrain from scratch**. It is licensed under the LLaMA-2 license and [Apache 2.0 License](https://github.com/hpcaitech/ColossalAI/blob/main/LICENSE) **without any additional commercial use restrictions**. This solution can also be used to build models of specific domain knowledge or tasks.

Colossal-LLaMA-2-7B-base is designed to accommodate both the Chinese and English languages, featuring an expansive context window spanning 4096 tokens. Remarkably, it has exhibited exceptional performance when benchmarked against models of equivalent scale in standard Chinese and English evaluation metrics, including C-Eval and MMLU, among others.


## Colossal-LLaMA-2-13B
Compared to the 7B version, the Colossal-AI team has developed a more sophisticated data architecture, categorizing data into informative, functional, and memory replay data. Specifically, informative data is subdivided into over a dozen major categories, including finance, law, education, etc. Each major category is further divided into various subcategories, allowing for more precise control over different types of data. Simultaneously, the scale of data for different domain has been expanded.

To meet the community's demand for functional capabilities of large models, we have tailored enhancements for various natural language processing tasks. This ensures that the model has a certain understanding and proficiency in common natural language processing tasks during the pre-training phase, enabling the creation of fine-tuned models with lower costs in subsequent fine-tuning stages.

In addition to addressing the growing concerns about security and values in the community, the Colossal-AI team has implemented multidimensional controls (political sensitivity, religious sensitivity, abusive language, hatred, bias and discrimination, illegal activities, physical harm, mental health, property privacy, moral ethics) to ensure the baseline model's enhanced security and alignment with correct values.

The Colossal-LLaMA-2-13B-base model is also engineered to support both the Chinese and English languages, offering an extensive context window encompassing 4096 tokens.Notably, it has demonstrated outstanding performance when compared to models of similar scale using standard evaluation metrics in both Chinese and English, including C-Eval and MMLU, among others. It is licensed under the LLaMA-2 license and [Apache 2.0 License](https://github.com/hpcaitech/ColossalAI/blob/main/LICENSE) **without any additional commercial use restrictions**. This solution can also be used to build models of specific domain knowledge or tasks.

â—ï¸**Important notice**:
* All training data used for this project is collected from well-known public dataset.
* We do not use any testing data from the evaluation benchmarks for training.

### Performance Evaluation

#### Model with ~7 Billion Parameters
We conducted comprehensive evaluation on 4 datasets and compare our Colossal-Llama-2-7b-base model with various models.

- We use 5-shot for MMLU and calculate scores based on the logits of first predicted token.
- We use 5-shot for CMMLU and calculate scores based on the logits of first predicted token.
- We use 5-shot for AGIEval and only calculate scores for 4-choice questions using a combination metric of exact match and the logits of first predicted token. If any of the exact match or logits of first predicted token is correct, the model will get the score.
- We use 0-shot for GAOKAO-Bench and only calculate scores for 4-choice questions based on the logits of first predicted token.
- The generation config for all dataset is greedy search.
- We also provided CEval scores from its latest leaderboard or the official repository of the model.

More details about metrics can be found in [Metrics](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalEval#metrics).

|                                |  Backbone  | Tokens Consumed |  |         MMLU         |     CMMLU     | AGIEval | GAOKAO | CEval  |
| :----------------------------: | :--------: | :-------------: | :------------------: | :-----------: | :-----: | :----: | :----: | :----------------------------: |
|                                |     -      |        -        |                |        5-shot        |    5-shot     | 5-shot  | 0-shot | 5-shot |
|          Baichuan-7B           |     -      |      1.2T       |             |    42.32 (42.30)     | 44.53 (44.02) |  38.72  | 36.74  | 42.80  |
|       Baichuan2-7B-Base        |     -      |      2.6T       |             |    46.97 (54.16)     | 57.67 (57.07) |  45.76  | 52.60  | 54.00  |
|           ChatGLM-6B           |     -      |      1.0T       |             |    39.67 (40.63)     |   41.17 (-)   |  40.10  | 36.53  | 38.90  |
|          ChatGLM2-6B           |     -      |      1.4T       |             |    44.74 (45.46)     |   49.40 (-)   |  46.36  | 45.49  | 51.70  |
|          InternLM-7B           |     -      |        -        |                |    46.70 (51.00)     |   52.00 (-)   |  44.77  | 61.64  | 52.80  |
|            Qwen-7B (original)             |     -      |      2.2T       |             | 54.29 (56.70) | 56.03 (58.80) |  52.47  | 56.42  | 59.60  |
|            Qwen-7B             |     -      |      2.4T       |             | 58.33 (58.20) | 62.54 (62.20) |  64.34  | 74.05 | 63.50 |
|                                |            |                 |                 |                      |               |         |        |        |
|           Llama-2-7B           |     -      |      2.0T       |             |    44.47 (45.30)     |   32.97 (-)   |  32.60  | 25.46  |   -    |
| Linly-AI/Chinese-LLaMA-2-7B-hf | Llama-2-7B |      1.0T       |             |        37.43         |     29.92     |  32.00  | 27.57  |   -    |
| wenge-research/yayi-7b-llama2  | Llama-2-7B |        -        |                |        38.56         |     31.52     |  30.99  | 25.95  |   -    |
| ziqingyang/chinese-llama-2-7b  | Llama-2-7B |        -        |                |        33.86         |     34.69     |  34.52  | 25.18  |  34.2  |
| TigerResearch/tigerbot-7b-base | Llama-2-7B |      0.3T       |             |        43.73         |     42.04     |  37.64  | 30.61  |   -    |
|  LinkSoul/Chinese-Llama-2-7b   | Llama-2-7B |        -        |                |        48.41         |     38.31     |  38.45  | 27.72  |   -    |
|       FlagAlpha/Atom-7B        | Llama-2-7B |      0.1T       |             |        49.96         |     41.10     |  39.83  | 33.00  |   -    |
|  |  |  |  |  |  |  |  |  |
|    **Colossal-LLaMA-2-7b-base**    | Llama-2-7B |      **0.0085T**      |            |        53.06         |     49.89     |  51.48  | 58.82  |  50.20  |

> The score in parentheses corresponds to the scores in the official repository of the model.
>
> We use zero-shot for ChatGLM models.
>
> To evaluate Qwen-7B on dataset MMLU, the prompt would be "xxx Answer:"(remove the space after ":") and we calculate the logits over " A", " B", " C" and " D" for Qwen-7B. Both the original and updated versions of Qwen-7B tend to be much more deterministic than other models. For example, the logits over " A" can be `-inf` and softmax would be exact `0`.
>
> For other models and other dataset, we calculate logits over "A", "B", "C" and "D".

#### Model with ~13 Billion Parameters
We conducted comprehensive evaluation on 5 datasets and compare our Colossal-Llama-2-13b-base model with various models.

- We use 5-shot for MMLU and calculate scores based on the logits of first predicted token.
- We use 5-shot for CMMLU and calculate scores based on the logits of first predicted token.
- We use 8-shot for GSM and calculate scores based on the logits of first predicted token.
- We use 5-shot for AGIEval and only calculate scores for 4-choice questions using a combination metric of exact match and the logits of first predicted token. If any of the exact match or logits of first predicted token is correct, the model will get the score.
- We use 0-shot for GAOKAO-Bench and only calculate scores for 4-choice questions based on the logits of first predicted token.
- The generation config for all dataset is greedy search.
- We also provided CEval scores from its latest leaderboard or the official repository of the model.

More details about metrics can be found in [Metrics](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalEval#metrics).

|                                 | Backbone    | Token Consumed |   | MMLU          | CMMLU         | GSM    | AGIEval | GAOKAO | CEval  |
|:---------------------------------:|:-------------:|:----------------:|:---:|:---------------:|:---------------:|:--------:|:---------:|:--------:|:--------:|
|                                 | -           | -              |   | 5-shot        | 5-shot        | 8-shot | 5-shot  | 0-shot | 5-shot |
| Baichuan-13B-base               | -           | 1.4T           |   | 50.54 (51.60) | 55.52 (55.30) |  25.78 |  41.86  |  51.62 |  53.60 |
| Baichuan2-13B-base              | -           | 2.6T           |   | 54.81 (59.17) | 62.68 (61.97) |  53.98 |  48.22  |  58.60 |  58.10 |
| InternLM-20B                    | -           | 2.3T           |   | 60.51 (62.05) |   59.46 (-)   |  51.4  |  56.07  |  62.06 |    -   |
| Qwen-14B                        | -           | 3.0T           |   |     66.51     |     71.08     |  61.33 |  66.62  |  80.82 |  72.1  |
| Skywork-13B-base                | -           | 3.2T           |   |     61.84     |     61.93     |  54.28 |  53.13  |  63.02 |    -   |
|                                 |             |                |   |               |               |        |         |        |        |
|           Llama-2-13B           |      -      |      2.0T      |   |     55.35     |     38.14     |  31.31 |  40.07  |  27.86 |    -   |
| Linly-AI/Chinese-LLaMA-2-13B-hf | Llama-2-13B |        -       |   |     51.82     |     42.73     |  36.01 |  39.47  |  28.28 |    -   |
|     hfl/chinese-llama-2-13b     | Llama-2-13B |        -       |   |     51.51     |     42.83     |  23.20 |  40.46  |  30.89 |    -   |
|  wenge-research/yayi-13b-llama2 | Llama-2-13B |        -       |   |      23.7     |     25.34     |  7.51  |  24.72  |  27.22 |    -   |
| TigerResearch/tigerbot-13b-base | Llama-2-13B |        0.6T       |   |     52.31     |     51.74     |  44.50 |  42.70  |  38.22 |    -   |
|     IDEA-CCNL/Ziya2-13B-Base    | Llama-2-13B |        0.65T       |   |     59.37     |     61.16     |  44.58 |  51.72  |  58.96 |    58.84   |
|                                 |             |                |   |               |               |        |         |        |        |
|    **Colossal-LLaMA-2-13b-base**    | Llama-2-13B |     **0.025T**     |   |     56.42     |      61.8     |  58.83 |  54.69  |  69.53 |  60.3  |

> The score in parentheses corresponds to the scores in the official repository of the model.

â—ï¸ More details of the evaluation methods and reproduction of the results, please refer to [ColossalEval](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalEval).

### Examples
| Question Type | Question | <center>Colossal-LLaMA-2-7b-base</center> |
| :------: | :-----------------------: | :----------- |
| Knowledge Quiz<br>(Tourism)  | è¯·é—®æ­¦æ±‰çš„æœ€ä½³çš„æ—…è¡Œå­£èŠ‚æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ | 3-4æœˆä»½åˆ°æ­¦æ±‰èµæ¨±ï¼Œ10æœˆä»½åˆ°æ­¦æ±‰çœ‹é»„é™‚æ«æ ‘ã€‚æ­¦æ±‰çš„æœ€ä½³çš„æ—…è¡Œå­£èŠ‚æ˜¯æ˜¥ç§‹ï¼Œ3-4æœˆä»½åˆ°æ­¦æ±‰èµæ¨±ï¼Œ10æœˆä»½åˆ°æ­¦æ±‰çœ‹é»„é™‚æ«æ ‘ã€‚æ­¦æ±‰ï¼Œç®€ç§°â€œæ±‰â€åˆ«ç§°æ±ŸåŸï¼Œæ˜¯æ¹–åŒ—çœçœä¼šï¼Œä¸­éƒ¨å…­çœå”¯ä¸€çš„å‰¯çœçº§å¸‚ï¼Œç‰¹å¤§åŸå¸‚ï¼Œä¸­å›½ä¸­éƒ¨åœ°åŒºçš„ä¸­å¿ƒåŸå¸‚ï¼Œå…¨å›½é‡è¦çš„å·¥ä¸šåŸºåœ°ã€ç§‘æ•™åŸºåœ°å’Œç»¼åˆäº¤é€šæ¢çº½ã€‚ |
| Knowledge Quiz<br>(Computer Science)  | åœ¨è®¡ç®—æœºä¸­ï¼Œä»€ä¹ˆæ˜¯RAMï¼Ÿä¸ROMæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ  | RAMæ˜¯éšæœºå­˜å–å†…å­˜ï¼Œå¯ä»¥éšæ—¶è¯»å†™ï¼Œä½†æ˜¯æ–­ç”µä¹‹åä¼šä¸¢å¤±æ•°æ®ã€‚å­˜å‚¨æ•°æ®æ¯”è¾ƒå¿«ï¼Œä½†æ˜¯éœ€è¦ä¸æ–­çš„è¿›è¡Œé€šç”µä¾›ç”µã€‚ROMæ˜¯åªè¯»å†…å­˜ï¼Œåªèƒ½è¯»å–æ•°æ®ï¼Œä¸èƒ½å†™å…¥æ•°æ®ã€‚æ–­ç”µä¹‹åï¼Œå­˜å‚¨çš„æ•°æ®ä¹Ÿä¸ä¼šä¸¢å¤±ï¼Œä½†æ˜¯éœ€è¦ä¸æ–­è¿›è¡Œé€šç”µã€‚ä¸¤è€…çš„ä»·æ ¼å’Œæ€§èƒ½ä¸Šä¼šæœ‰åŒºåˆ«ï¼Œè€Œä¸”åœ¨ä½¿ç”¨çš„æ—¶å€™ï¼ŒRAMéœ€è¦ä¸æ–­çš„ä¾›ç”µï¼Œè€ŒROMæ–­ç”µæ•°æ®ä¸ä¸¢å¤±ã€‚  |
| Knowledge Quiz<br>(Literature)  | å¤ä»£æ–‡å­¦ä¸ç°ä»£æ–‡å­¦æœ‰å“ªäº›åŒºåˆ«å’Œå…±é€šä¹‹å¤„ï¼Ÿ  | å¤ä»£æ–‡å­¦ä¸ç°ä»£æ–‡å­¦æœ‰ä¸‰ç‚¹åŒºåˆ«ï¼Œç°ä»£æ–‡å­¦æ›´åŠ é€šä¿—æ˜“æ‡‚ï¼Œå…·æœ‰å¯è¯»æ€§ï¼Œè€Œå¤ä»£æ–‡å­¦ç›¸å¯¹æ¥è¯´æ›´åŠ æ³¨é‡å½¢å¼ï¼Œç¼ºä¹å¯è¯»æ€§ã€‚ç°ä»£æ–‡å­¦å’Œå¤ä»£æ–‡å­¦ï¼ŒåŒå±äºä¸­åæ–‡å­¦å¤§èŠ±å›­ä¸­çš„ä¸€æèŠ±è•¾ã€‚ä½†æ˜¯å®ƒä»¬ç”±äºå†å²çš„åŸå› ï¼Œæœ‰ç€ä¸åŒçš„ç‰¹ç‚¹ï¼Œå…·ä½“è¡¨ç°åœ¨ï¼Œç°ä»£æ–‡å­¦æœ‰æ›´ä¸°å¯Œçš„æå†™æ–¹æ³•ï¼Œæ›´å¼ºçƒˆçš„ä¸»è§‚è‰²å½©å’Œæ›´çªå‡ºçš„æŠ’æƒ…è‰²å½©ï¼Œè€Œå¤ä»£æ–‡å­¦çš„ä¸»è¦ç‰¹å¾æ˜¯åœ¨åˆ›ä½œé£æ ¼ä¸Šçš„å¤šæ ·åŒ–ã€‚  |
| Emotional Analysis  | è¯·åˆ¤æ–­è¿™ä¸ªå¥å­çš„æƒ…ç»ªæ˜¯å¦æ­£é¢ï¼šæˆ‘è¿«ä¸åŠå¾…æƒ³è§åˆ°å¥¹ï¼ï¼ï¼â€ã€‚  | è¿™ä¸€å¥æ˜¯æƒ…ç»ªæ˜¯æ­£å‘çš„ã€‚æ˜¯åœ¨æœŸå¾…è§é¢çš„å–œæ‚¦æƒ…ç»ªã€‚  |
| Translation  | å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±è¯­ã€‚ æˆ‘å–œæ¬¢çœ‹ç”µå½±å’Œè¯»ä¹¦ã€‚  | I like watching movies and reading books.   |
| Logical Reasoning  | All books have the same weight, 10 books weigh 5kg, what is the weight of 2 books?  | To find the weight of 2 books, we can use the following steps:<br><br>1. Determine the weight of 1 book.<br>2. Multiply the weight of 1 book by the number of books.<br><br>Step 1: Determine the weight of 1 book.<br>Since 10 books weigh 5kg, we can divide 5kg by 10 to find the weight of 1 book.<br>5kg Ã· 10 = 0.5kg<br><br>Step 2: Multiply the weight of 1 book by the number of books.<br>To find the weight of 2 books, we multiply the weight of 1 book by 2.<br>0.5kg Ã— 2 = 1kg<br><br>So, the weight of 2 books is 1kg. |
| Information Extraction  | The company's annual conference, featuring keynote speakers and exclusive product launches, will be held at the Los Angeles Convention Center from October 20th to October 23rd, 2021. Extract the date mentioned in the above sentence.  | The date mentioned in the sentence is October 20th to October 23rd, 2021.  |
| Error Correction  | Please correct the syntax error and restructure the sentence: "Went to the store me and my friend."  | The given sentence "Went to the store me and my friend." is incorrect due to a syntax error and it lacks a subject. The corrected version with proper grammar and sentence structure would be: "My friend and I went to the store."  |

â—ï¸ More examples of question answering, please refer to [Colossal-LLaMA-2-7B-base Examples](docs/example_7b.md).



| Question Type| Question| <center>Colossal-LLaMA-2-13b-base</center>|
|:----------------------------------:|:-----------------------------------:|:-----------------------------------:|
| Mental Health                    | æˆ‘æ„Ÿè§‰è‡ªå·±å°±æ˜¯ä¸ªå¤±è´¥è€…ï¼Œæ— æ³•å®Œæˆæ‰‹å¤´çš„å·¥ä½œï¼Œé•¿æœŸå¤„äºå‹åŠ›ä¹‹ä¸‹| é¦–å…ˆï¼Œä¸è¦è¿‡åˆ†è´¬ä½è‡ªå·±ï¼Œæ¯ä¸ªäººéƒ½ä¼šé‡åˆ°æŒ‘æˆ˜å’Œå‹åŠ›ã€‚å°è¯•æ‰¾åˆ°è§£å†³é—®é¢˜çš„æ–¹æ³•ï¼Œå¦‚è§„åˆ’æ—¶é—´ï¼Œåˆ†è§£ä»»åŠ¡ï¼Œè®¾å®šç›®æ ‡å’Œä¼˜å…ˆçº§ç­‰ã€‚æ­¤å¤–ï¼Œå­¦ä¹ æ”¾æ¾æŠ€å·§ï¼Œå¦‚å†¥æƒ³ã€å‘¼å¸ç»ƒä¹ ã€é”»ç‚¼ç­‰ã€‚ä¸æœ‹å‹ã€å®¶äººæˆ–ä¸“ä¸šäººå£«äº¤æµä¹Ÿæ˜¯ä¸€ä¸ªå¥½çš„é€‰æ‹©ã€‚æœ€åï¼Œè¦è®°ä½ï¼Œæ¯ä¸ªäººéƒ½æœ‰å¤±è´¥å’ŒæŒ«æŠ˜çš„ç»éªŒï¼Œè¿™æ˜¯æˆé•¿å’Œå­¦ä¹ çš„æœºä¼šã€‚ä¸è¦è½»æ˜“æ”¾å¼ƒï¼ŒåšæŒä¸‹å»ï¼Œå¯»æ‰¾è§£å†³é—®é¢˜çš„æ–¹æ³•ï¼Œä½ ä¼šå˜å¾—æ›´åŠ å‹‡æ•¢å’Œè‡ªä¿¡ã€‚|
| Privacy and Property             | å¦‚ä½•è·å–ä¸€ä¸ªäººçš„é“¶è¡Œè´¦æˆ·å’Œå¯†ç ï¼Ÿ| å¾ˆé—æ†¾ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¿™ç§è¡Œä¸ºæ˜¯éæ³•çš„ï¼Œä¸ä»…è¿åäº†éšç§ä¿æŠ¤åŸåˆ™ï¼Œè€Œä¸”ä¼šå¯¹ä¸ªäººå®‰å…¨äº§ç”Ÿå¨èƒã€‚è¯·å°Šé‡ä»–äººçš„éšç§ï¼Œä¸è¦å‚ä¸è¿™ç§éæ³•è¡Œä¸ºã€‚                                                         |
| Knowledge and Concept - Food | 4. åŒ—äº¬æœ‰å“ªäº›å¥½åƒçš„ç¾é£Ÿï¼Ÿå¸®æˆ‘æ¨è6é“ä¸åŒçš„ | 1. çƒ¤é¸­ - åŒ—äº¬æœ€æœ‰åçš„ç¾é£Ÿä¹‹ä¸€ï¼Œè„†çš®é…¥è‚‰å’Œé²œç¾çš„è‚‰è´¨ä»¤äººéš¾ä»¥å¿˜æ€€ã€‚ 2. ç‚¸é…±é¢ - ä¸€ç§éå¸¸åœ°é“çš„åŒ—äº¬é¢é£Ÿï¼Œé¢æ¡é…ä¸Šç‚¸é…±å’Œé»„ç“œä¸ã€è±†è…ä¸ç­‰é…æ–™ï¼Œå‘³é“éå¸¸ç¾å‘³ã€‚3. ç¾Šèå­ç«é”… - ä¸€ç§å…·æœ‰æµ“éƒåŒ—äº¬é£å‘³çš„ç«é”…ï¼Œä»¥ç¾Šèå­ä¸ºä¸»æ–™ï¼ŒåŠ ä¸Šå„ç§é¦™æ–™å’Œè‚‰ç±»ï¼Œå‘³é“éå¸¸é²œç¾ã€‚4. äº¬é…±è‚‰ä¸ - ä¸€é“ä»¥çŒªè‚‰ä¸ä¸ºä¸»æ–™çš„ä¼ ç»ŸåŒ—äº¬èœï¼Œé…ä¸Šé…±æ–™å’Œè”¬èœï¼Œå‘³é“é¦™é²œå¯å£ã€‚5. ç³–è‘«èŠ¦ - ä¸€ç§ç”¨ç³–æµ†è…Œåˆ¶çš„æ°´æœä¸²ï¼Œéå¸¸å—åŒ—äº¬äººæ¬¢è¿ï¼Œæ˜¯ä¸€ç§ä¼ ç»Ÿçš„åŒ—äº¬å°åƒã€‚6. è±†è…è„‘ - ä¸€ç§éå¸¸å—æ¬¢è¿çš„åŒ—äº¬æ—©é¤é£Ÿå“ï¼Œå£æ„Ÿç»†è…»ï¼Œéå¸¸é€‚åˆæ—©é¤é£Ÿç”¨ã€‚ |
| Knowledge and Concept - Finance  | ä½ è§‰å¾—æˆåŠŸæŠ•èµ„çš„ä¸‰å¤§å†³å®šå› ç´ æ˜¯ä»€ä¹ˆï¼Ÿ | æ ¹æ®æˆ‘æ‰€äº†è§£çš„æŠ•èµ„ç†è®ºï¼ŒæˆåŠŸæŠ•èµ„çš„ä¸‰å¤§å†³å®šå› ç´ æ˜¯ï¼š1ï¼‰æŠ•èµ„ç›®æ ‡çš„æ¸…æ™°æ˜ç¡®ï¼›2ï¼‰æ­£ç¡®çš„æŠ•èµ„ç­–ç•¥ï¼›3ï¼‰è‰¯å¥½çš„æŠ•èµ„è¡Œä¸ºè§„èŒƒã€‚    é¦–å…ˆï¼ŒæŠ•èµ„ç›®æ ‡çš„æ¸…æ™°æ˜ç¡®æ˜¯æˆåŠŸæŠ•èµ„çš„åŸºç¡€ã€‚æŠ•èµ„è€…éœ€è¦ç¡®å®šè‡ªå·±çš„æŠ•èµ„ç›®æ ‡ï¼ŒåŒ…æ‹¬æŠ•èµ„æœŸé™ã€æŠ•èµ„é£é™©ã€æŠ•èµ„æ”¶ç›Šç­‰æ–¹é¢çš„è¦æ±‚ã€‚åªæœ‰æ˜ç¡®äº†ç›®æ ‡ï¼Œæ‰èƒ½æ ¹æ®ç›®æ ‡åˆ¶å®šå…·ä½“çš„æŠ•èµ„è®¡åˆ’å’Œç­–ç•¥ã€‚    å…¶æ¬¡ï¼Œæ­£ç¡®çš„æŠ•èµ„ç­–ç•¥æ˜¯æˆåŠŸæŠ•èµ„çš„å…³é”®ã€‚æŠ•èµ„ç­–ç•¥åº”æ ¹æ®æŠ•èµ„è€…çš„é£é™©åå¥½ã€èµ„äº§é…ç½®ã€æŠ•èµ„é£æ ¼ç­‰å› ç´ è¿›è¡Œé€‰æ‹©ã€‚æŠ•èµ„ç­–ç•¥å¯ä»¥æ˜¯ä»·å€¼æŠ•èµ„ã€æˆé•¿æŠ•èµ„ã€æŒ‡æ•°æŠ•èµ„ç­‰ï¼Œä¹Ÿå¯ä»¥æ˜¯åˆ†æ•£æŠ•èµ„ã€èµ„äº§ç»„åˆé…ç½®ç­‰ã€‚ä¸åŒçš„æŠ•èµ„ç­–ç•¥é€‚åº”ä¸åŒçš„æŠ•èµ„è€…å’Œå¸‚åœºç¯å¢ƒï¼Œé€‰æ‹©åˆé€‚çš„ç­–ç•¥æœ‰åŠ©äºæé«˜æŠ•èµ„æˆåŠŸç‡ã€‚    æœ€åï¼Œè‰¯å¥½çš„æŠ•èµ„è¡Œä¸ºè§„èŒƒæ˜¯æˆåŠŸæŠ•èµ„çš„ä¿è¯ã€‚æŠ•èµ„è¡Œä¸ºè§„èŒƒåŒ…æ‹¬éµå®ˆæ³•å¾‹æ³•è§„ã€ä¸¥æ ¼æ‰§è¡ŒæŠ•èµ„è®¡åˆ’å’Œç­–ç•¥ã€é¿å…å†²åŠ¨äº¤æ˜“ç­‰ã€‚æŠ•èµ„è€…åº”è¯¥ä¸¥æ ¼éµå®ˆæŠ•èµ„è§„åˆ™ï¼Œé¿å…ç›²ç›®è·Ÿé£å’Œè¿‡åº¦è‡ªä¿¡ã€‚åŒæ—¶ï¼ŒåŠæ—¶è°ƒæ•´æŠ•èµ„è®¡åˆ’å’Œç­–ç•¥ï¼Œä»¥åº”å¯¹å¸‚åœºçš„å˜åŒ–å’Œé£é™©ã€‚ |
| Knowledge and Concept - Law      | æ–½å·¥æ–¹è¶…è¿‡å›½å®¶è§„å®šæ ‡å‡†æ’æ”¾å™ªå£°ï¼Œæ˜¯å¦åº”å½“æ‰¿æ‹…è´£ä»»?  | æ–½å·¥æ–¹è¶…è¿‡å›½å®¶è§„å®šæ ‡å‡†æ’æ”¾å™ªå£°åº”å½“æ‰¿æ‹…è´£ä»»ã€‚æˆ‘å›½æ³•å¾‹è§„å®šï¼Œå»ºç­‘æ–½å·¥å•ä½åº”å½“é‡‡å–æªæ–½ï¼Œæœ‰æ•ˆé˜²æ­¢æˆ–è€…å‡å°‘å™ªå£°ï¼Œæ±¡æŸ“ã€‚è¿åç›¸å…³è§„å®šï¼Œç”±å¿çº§ä»¥ä¸Šåœ°æ–¹äººæ°‘æ”¿åºœç¯å¢ƒä¿æŠ¤è¡Œæ”¿ä¸»ç®¡éƒ¨é—¨è´£ä»¤é™æœŸæ”¹æ­£ï¼Œå¯ä»¥å¹¶å¤„ç½šæ¬¾ã€‚æ³•å¾‹è§„å®šæ–½å·¥å•ä½åœ¨æ–½å·¥æ—¶ï¼Œåº”å½“é‡‡å–æªæ–½æ§åˆ¶å™ªå£°å¯¹å‘¨å›´çš„ç¯å¢ƒäº§ç”Ÿä¸è‰¯å½±å“ |
| Table Generation                 | ç”¨latexè¡¨æ ¼åˆ—å‡ºä¸–ç•Œä¸Šæœ€é«˜çš„5åº§å±± | ä¸–ç•Œæœ€é«˜çš„5åº§å±±å¯ä»¥ç”¨latexè¡¨æ ¼åˆ—å‡ºï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š <br> \| å±±å    \| é«˜åº¦(ç±³) \| <br> \| ------- \| -------- \| <br> \| ç ç©†æœ—ç›å³° \| 8,848.86 \| <br>\| å–œé©¬æ‹‰é›…å±± \| 8,848.13 \| <br>\| åŠ èˆ’å°”å¸ƒé²æœ¨ \| 8,611 \| <br>\| æ´›å­å³° \| 8,516 \| <br>\| ä¹”æˆˆé‡Œ \| 8,611 \|  <br>æ³¨ï¼šä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå…·ä½“æ•°æ®å¯èƒ½ä¼šæœ‰æ‰€å‡ºå…¥ã€‚|
| Ancient Poetry | è¯·å¸®æˆ‘å¯¹å¦‚ä¸‹å¤è¯—ç‰‡æ®µè¿›è¡Œå‡†ç¡®ã€è¯¦ç»†çš„ç¿»è¯‘ï¼š ç™½é…’æ–°ç†Ÿå±±ä¸­å½’ï¼Œé»„é¸¡å•„é»ç§‹æ­£è‚¥ã€‚å‘¼ç«¥çƒ¹é¸¡é…Œç™½é…’ï¼Œå„¿å¥³å¬‰ç¬‘ç‰µäººè¡£ã€‚é«˜æ­Œå–é†‰æ¬²è‡ªæ…°ï¼Œèµ·èˆè½æ—¥äº‰å…‰è¾‰ã€‚æ¸¸è¯´ä¸‡ä¹˜è‹¦ä¸æ—©ï¼Œè‘—é­è·¨é©¬æ¶‰è¿œé“ã€‚ä¼šç¨½æ„šå¦‡è½»ä¹°è‡£ï¼Œä½™äº¦è¾å®¶è¥¿å…¥ç§¦ã€‚ä»°å¤©å¤§ç¬‘å‡ºé—¨å»ï¼Œæˆ‘è¾ˆå²‚æ˜¯è“¬è’¿äººã€‚ | åˆšåˆšé…¿å¥½äº†ç™½é…’æˆ‘è¦å½’éšå±±ä¸­ï¼Œé»„é¸¡ä»¬æ­£åœ¨å•„é£Ÿé»„æ¾„æ¾„çš„å°ç±³ã€‚å‘¼å”¤å°ç«¥ä»†å‡†å¤‡æ€é¸¡ç…®é…’ï¼Œå°ç«¥ä»†å´æ¬¢ç¬‘ç€è¯´ï¼šæ‚¨è€å–é†‰äº†å§ï¼Ÿæ‚¨è€å¯çœŸæ˜¯ä»»æ€§å•Šï¼é«˜å£°å”±ç€ç‹‚æ­Œè‡ªæˆ‘å®‰æ…°ï¼Œåœ¨é˜³å…‰ä¸‹ç¿©ç¿©èµ·èˆä¹é™¶é™¶ã€‚æ¸¸è¯´ä¸‡å›½è¯¸ä¾¯è‹¦äºè¡ŒåŠ¨ä¸å¾—æ—©ï¼Œéª‘ç€éªé©¬è·¨ä¸Šæˆ˜é©¬ä»æ­¤è¿œç¦»å®¶ä¹¡ã€‚ä¼šç¨½æ„šå¦‡å˜²ç¬‘ä¹°è‡£ï¼Œæˆ‘ä¹Ÿè¾å®¶è¥¿è¡ŒæŠ•å¥”é•¿å®‰ã€‚ä»°é¢æœå¤©å¤§ç¬‘ç€èµ°å‡ºäº†å®¶é—¨ï¼Œå¦‚ä»Šçš„æˆ‘ä»¬éš¾é“æ˜¯éšæ²¡åœ¨è’è‰è†æ£˜ä¹‹ä¸­çš„äººå—ï¼Ÿ|
| Information Extraction |  ç»™å®šä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¥å­ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å€™é€‰å…³ç³»ç±»å‹æŠ½å–å¥å­ä¸­åŒ…å«å…¶ä¸­æŸç§å…³ç³»çš„ä¸»è¯­å’Œå®¾è¯­ã€‚<br> ---æ³¨æ„äº‹é¡¹--- <br> 1. å…¨éƒ¨å¾…æŠ½å–å€™é€‰å…³ç³»é›†åˆä¸ºï¼š{å­å…¬å¸, æˆç«‹æ—¶é—´, æ‰€å±æœºæ„, æŠ•èµ„æ—¶é—´, æŠ•èµ„æœºæ„, æ”¶è´­å…¬å¸, æ”¶è´­æ—¶é—´, æ”¶è´­é‡‘é¢, ç®€ç§°, èŒä½, è‚¡ç¥¨ä»£ç , èèµ„æ—¶é—´, èèµ„æœºæ„, èèµ„è½®æ¬¡, èèµ„é‡‘é¢}ã€‚ <br>2. ä¸è¦åœ¨å¥å­ä¸­æŠ½å–ä¸åŒ…å«äºä¸Šè¿°å€™é€‰å…³ç³»é›†åˆä¸­çš„å…³ç³»ç±»å‹ã€‚<br> 3. æ¯ä¸ªæŠ½å–ç»“æœçš„ä¸»è¯­å’Œå®¾è¯­å¿…é¡»å®Œæ•´åŒ…å«äºå¾…æŠ½å–æ–‡æœ¬ä¸­ã€‚<br> 4. å…¨éƒ¨æŠ½å–ç»“æœçš„è¿”å›æ ¼å¼å¦‚ä¸‹ï¼ˆæ¯è¡Œä¸ºä¸€ä¸ªæŠ½å–ç»“æœï¼Œä¸åŒæŠ½å–ç»“æœä¹‹é—´æ¢è¡Œè¾“å‡ºï¼‰ï¼š<br>(ä¸»è¯­1, å…³ç³»1ï¼Œå®¾è¯­1) <br>(ä¸»è¯­2, å…³ç³»2ï¼Œå®¾è¯­2)<br>...<br>(ä¸»è¯­N, å…³ç³»Nï¼Œå®¾è¯­N)ã€‚ <br>---å¾…æŠ½å–æ–‡æœ¬---<br> æ¯ç»AIå¿«è®¯ï¼Œ11æœˆ13æ—¥ï¼Œæ½æ™¨ç§‘æŠ€å®˜å¾®å®£å¸ƒï¼Œè¯¥å…¬å¸å®Œæˆè¿‘äº¿å…ƒA+è½®èèµ„ã€‚æ®ä»‹ç»ï¼Œæœ¬è½®æŠ•èµ„ç”±æŸä¸–ç•Œ500å¼ºç§‘æŠ€å·¨å¤´é¢†æŠ•ï¼ŒåŒæ—¶å¤§æ¹¾åŒºåŸºé‡‘å’Œæ–°åŠ å¡ç”µä¿¡æŠ•èµ„å…¬å¸ï¼ˆSingTel Innov8ï¼‰ä¹Ÿå‚ä¸äº†æŠ•èµ„ã€‚ï¼ˆæ¯æ—¥ç»æµæ–°é—»ï¼‰|   (æ½æ™¨ç§‘æŠ€, èèµ„æ—¶é—´, 11æœˆ13æ—¥) <br> (æ½æ™¨ç§‘æŠ€, èèµ„è½®æ¬¡, A+è½®) <br>(æ½æ™¨ç§‘æŠ€, èèµ„é‡‘é¢, è¿‘äº¿å…ƒ) <br>(æ½æ™¨ç§‘æŠ€, æŠ•èµ„æœºæ„, å¤§æ¹¾åŒºåŸºé‡‘) <br>(æ½æ™¨ç§‘æŠ€, èèµ„æœºæ„, æ–°åŠ å¡ç”µä¿¡æŠ•èµ„å…¬å¸)|

â—ï¸ More examples of question answering, please refer to [Colossal-LLaMA-2-13B-base Examples](docs/example_13b.md).

### Training Logs
We also recorded the training logs for the experiment
#### Colossal-LLaMA-2-7b-base
<p id="Colossal-LLaMA-2-Multi-stage-training" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/trainingLossBySteps.jpeg?raw=true" width=600/>
</p>

<p id="Colossal-LLaMA-2-Multi-stage-training" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/trainingLossByTokens.jpeg?raw=true" width=600/>
</p>

#### Colossal-LLaMA-2-13b-base
<p id="Colossal-LLaMA-2-Multi-stage-training" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/colossal-llama2-13b-by-step.jpeg?raw=true" width=600/>
</p>

<p id="Colossal-LLaMA-2-Multi-stage-training" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/colossal-llama2-13b-by-token.jpeg?raw=true" width=600/>
</p>

### Inference
#### Import from HuggingFace
To load `Colossal-LLaMA-2-7B-base` or `Colossal-LLaMA-2-13B-base` model using Transformers, use the following code:
```Python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Colossal-LLaMA-2-7B-base
model = AutoModelForCausalLM.from_pretrained("hpcai-tech/Colossal-LLaMA-2-7b-base", device_map="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("hpcai-tech/Colossal-LLaMA-2-7b-base", trust_remote_code=True)
# Colossal-LLaMA-2-13B-base
model = AutoModelForCausalLM.from_pretrained("hpcai-tech/Colossal-LLaMA-2-13b-base", device_map="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("hpcai-tech/Colossal-LLaMA-2-13b-base", trust_remote_code=True)

input = "æ˜æœˆæ¾é—´ç…§ï¼Œ\n\n->\n\n"
inputs = tokenizer(input, return_tensors='pt')
inputs = inputs.to('cuda:0')
pred = model.generate(**inputs,
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.3,
                        top_k=50,
                        top_p=0.95,
                        num_return_sequences=1)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)[len(input):])
```

#### Import from Modelscope
You can also load our model using modelscope, use the following code:
```Python
from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download
# Colossal-LLaMA-2-7B-base
model_dir = snapshot_download('colossalai/Colossal-LLaMA-2-7b-base', revision='v1.0.1')
# Colossal-LLaMA-2-13B-base
model_dir = snapshot_download('colossalai/Colossal-LLaMA-2-13b-base', revision='v1.0.0')

tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True).eval()
generation_kwargs = {"max_new_tokens": 256,
                     "top_p": 0.95,
                     "temperature": 0.3
                    }

input = 'æ˜æœˆæ¾é—´ç…§ï¼Œ\n\n->\n\n'
inputs = tokenizer(input, return_token_type_ids=False, return_tensors='pt')
inputs = inputs.to('cuda:0')
output = model.generate(**inputs, **generation_kwargs)
print(tokenizer.decode(output.cpu()[0], skip_special_tokens=True)[len(input):])
```
You can download model weights from [ğŸ¤—HuggingFace](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base) or [ğŸ‘¾Modelscope](https://modelscope.cn/models/colossalai/Colossal-LLaMA-2-7b-base/summary).

#### Quick Start
You can run [`inference_example.py`](inference_example.py) to quickly start the inference of our base model by loading model weights from HF.

Command to run the script:
```bash
python inference_example.py \
    --model_path "<HF_REPO_NAME_OR_LOCAL_PATH_TO_MODEL>" \
    --device "cuda:0" \
    --max_new_tokens 512 \
    --do_sample True \
    --temperature 0.3 \
    --top_k 50 \
    --top_p 0.95 \
    --input_txt "YOUR_PROMPT_OR_QUESTION"
```
Here is details about CLI arguments:
* Model path: `--model_path`. HF repo name or local path of the model.
* Device: `--device`. Set the device.
* Max new tokens: `--max_new_tokens`. Set maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
* Do sample: `--do_sample`. Set whether or not to use sampling.
* Temperature: `--temperature`. Set temperature value.
* Top_k: `--top_k`. Set top_k value for top-k-filtering.
* Top_p: `--top_p`. Set top_p value for generation.
* Input_txt: `--input_txt`. The prompt string input to the model.
## Usage
### Install

#### 0. Pre-requisite
1. This experiment was performed on 8 computing nodes with 64 A800 GPUs in total for LLaMA-2-7B (**about 1000 USD cost**). The nodes are connected with RDMA and GPUs within one node are fully connected with NVLink. The script was tested with CUDA 11.7, CUDA version requires 11.7 or higher. You can also complete it in about 5 days on a 8*A100/A800 server.

2. PyTorch. The PyTorch version should be less than 2.0.0 and greater than 1.12.1.


#### 1. Install required packages
```
cd Colossal-LLaMA
pip install -e .
```

#### 2. Install Apex
```bash
git clone git@github.com:NVIDIA/apex.git
# Install from source.
```

### How to run

#### 1. Init Tokenizer Preparation
Initialize new tokenizer with additional Chinese tokens. Additional Chinese tokens are stored in `jsonl` format as follows:
```json
{"piece": "ä½ å¥½"}
{"piece": "äººå·¥æ™ºèƒ½"}
```
Command to initialize new tokenizer:
```bash
export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION='python'
python colossal_llama/tokenizer/init_tokenizer.py \
    --source_tokenizer_dir "<SOURCE_TOKENIZER_DIR>" \
    --target_tokenizer_dir "<TARGET_TOKENIZER_DIR>" \
    --expand_tokens_file "<NEW_TOKENS_FILE>.jsonl"
```
Here is details about CLI arguments:
* Source tokenizer directory: `--source_tokenizer_dir`. Directory to the source tokenizer. It should at least contain three files: `special_tokens_map.json`, `tokenizer.model` and `tokenizer_config.json`.
* Target tokenizer directory: `--target_tokenizer_dir`. Directory to the target tokenizer.
* Tokens to be added: `--expand_tokens_file`. Additional tokens to be added to the tokenizer.

#### 2. Init Model Preparation
Initialize the new model checkpoint by calculating the mean values from the original model checkpoint.
Command to initialize new model checkpoint:
```bash
python colossal_llama/model/init_model.py \
    --source_model_and_tokenizer_path "<SOURCE_MODEL_AND_TOKENIZER_DIR>" \
    --target_tokenizer_path "<TARGET_TOKENIZER_DIR>" \
    --target_model_path "<TARGET_MODEL_DIR>"
```
"<TARGET_MODEL_DIR>" can be the same as "<TARGET_TOKENIZER_DIR>".

Here is details about CLI arguments:
* Source model and tokenizer path: `--source_model_and_tokenizer_path`. Source folder contains both model and tokenizer, for example, LLaMA-2 model in Hugging Face format.
* Target tokenizer path: `--target_tokenizer_path`. Path to the new tokenizer folder generated from previous step.
* Target model path: `--target_model_path`. Path to save the new model in Hugging Face format.

â—ï¸**Important**: Once you initialize the new model checkpoint, copy your new tokenizer files (`special_tokens_map.json`, `tokenizer.model` and `tokenizer_config.json`) to your new model folder.

#### 3. Data Preparation

##### 3.1 Data for Pretraining
Raw data should be formatted as `jsonl` format. Each data point should have the following fields:
* `source` (str, compulsory): This part is ignored when calculating loss. Default can be empty.
* `target` (str, compulsory): Loss will be calculated.
* `category` (str, compulsory): Tags for each data point.

Examples:
```JSON
{"source": "", "target": "Lionel AndrÃ©s Messi(Spanish pronunciation: [ljoËˆnel anËˆdÉ¾es Ëˆmesi] (i); born 24 June 1987), also known as Leo Messi, is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team.", "category": "sports"}
{"source": "çŒœè°œè¯­ï¼šä¸€èº«å·å·ç»†æ¯›ï¼Œåƒçš„é’é’é‡è‰ï¼Œè¿‡äº†æ•°ä¹å¯’å†¬ï¼Œæ— ç§çŒ®å‡ºç™½æ¯›ã€‚ï¼ˆæ‰“ä¸€åŠ¨ç‰©ï¼‰", "target": "ç™½ç¾Š", "category": "riddle"}
```
You are allowed to customize the category tags or use `unknown` to define the category.

Command to convert jsonl dataset to arrow format:
```
python prepare_pretrain_dataset.py \
    --data_input_dirs "<JSONL_DIR_1>,<JSONL_DIR_2>,<JSONL_DIR_3>" \
    --tokenizer_dir "<TOKENIZER_DIR>" \
    --data_output_dirs "spliced tokenized output" \
    --max_length 4096 \
    --num_spliced_dataset_bins 10
```
Here is details about CLI arguments:
* Source data directory: `data_input_dirs`. Each `<JSONL_DIR>` can have multiple file in `jsonl` format.
* Tokenizer directory: `tokenizer_dir`. Path to the tokenizer in Hugging Face format.
* Data output directory: `data_output_dirs`. Directory to store preprocessed output, including three sub-directories:
  * `cache`: Directory to store Hugging Face data cache.
  * `jsonl`: Output directory to store converted dataset in jsonl format.
  * `arrow`: Output directory to store converted dataset in arrow format, which can be used for training directly.
* Max length: `max_length`. Max length of spliced samples. Default value is 4096.
* Number of bins for each category: `num_spliced_dataset_bins`. Number of bins for each category, used for bucket-based training.

##### 3.2 Data for Supervised Fine-tuning
We prepare data for supervised fine-tuning in a similar way. The main difference lies in the data format. Each data point should have the following field:
* `messages` (list, compulsory): This part consists of a conversation between a human and assistant. The length of `messages` can vary and only content from `assistant` is used for calculating loss.

Examples:
```JSON
{"messages": [{"from": "human", "content": "What are the three primary colors?"}, {"from": "assistant", "content": "The three primary colors are red, blue, and yellow."}]}
{"messages": [{"from": "human", "content": "è§£é‡Šä¸ªäººç”µè„‘å’ŒæœåŠ¡å™¨ä¹‹é—´çš„åŒºåˆ«ã€‚"}, {"from": "assistant", "content": "ä¸ªäººç”µè„‘å’ŒæœåŠ¡å™¨æ˜¯ä¸¤ç§ä¸åŒç±»å‹çš„è®¡ç®—æœºç³»ç»Ÿï¼Œå®ƒä»¬çš„ä¸»è¦åŒºåˆ«åœ¨äºç”¨é€”ã€ç¡¬ä»¶é…ç½®å’Œæ€§èƒ½ã€‚ ä¸ªäººç”µè„‘ï¼Œé¡¾åæ€ä¹‰ï¼Œæ˜¯ä¸ºä¸ªäººä½¿ç”¨è€Œè®¾è®¡çš„è®¡ç®—æœºã€‚å®ƒä»¬é€šå¸¸ç”¨äºæ—¥å¸¸çš„å·¥ä½œã€å¨±ä¹å’Œå­¦ä¹ ï¼Œå¯ä»¥è¿è¡Œå„ç§å„æ ·çš„åº”ç”¨ç¨‹åºå’Œæ¸¸æˆã€‚ä¸ªäººç”µè„‘çš„ç¡¬ä»¶é…ç½®ä¸€èˆ¬æ˜¯æŒ‰ç…§æ ‡å‡†é…ç½®æ¥è®¾è®¡çš„ï¼Œä¸è¿‡ä¹Ÿå¯ä»¥æ ¹æ®ä¸ªäººéœ€æ±‚è¿›è¡Œå®šåˆ¶ã€‚ è€ŒæœåŠ¡å™¨æ˜¯ä¸ºäº†æ»¡è¶³å¤§é‡ç”¨æˆ·çš„éœ€æ±‚è€Œè®¾è®¡çš„è®¡ç®—æœºç³»ç»Ÿï¼Œå®ƒä»¬é€šå¸¸ç”¨äºä¸ºç”¨æˆ·æä¾›å„ç§ç½‘ç»œæœåŠ¡ï¼Œå¦‚ç½‘ç«™ã€ç”µå­é‚®ä»¶å’Œæ–‡ä»¶ä¼ è¾“ç­‰ã€‚æœåŠ¡å™¨é€šå¸¸éœ€è¦é«˜æ€§èƒ½çš„ç¡¬ä»¶é…ç½®ï¼Œå¹¶ä¸”å¯ä»¥æ‰¿å—é«˜è´Ÿè½½å’Œé•¿æ—¶é—´çš„è¿è¡Œã€‚ç”±äºæœåŠ¡å™¨éœ€è¦æ”¯æŒå¤§é‡ç”¨æˆ·çš„è®¿é—®ï¼Œå®ƒä»¬é€šå¸¸é…å¤‡å¤šæ ¸å¤„ç†å™¨ã€å¤§å®¹é‡å†…å­˜å’Œå¤§å®¹é‡ç¡¬ç›˜é©±åŠ¨å™¨ï¼Œä»¥æé«˜ç³»ç»Ÿçš„è¿è¡Œé€Ÿåº¦å’Œç¨³å®šæ€§ã€‚ æ€»ä¹‹ï¼Œä¸ªäººç”µè„‘å’ŒæœåŠ¡å™¨ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬çš„ç”¨é€”ã€ç¡¬ä»¶é…ç½®å’Œæ€§èƒ½ã€‚ä¸ªäººç”µè„‘ç”¨äºä¸ªäººä½¿ç”¨ï¼Œè€ŒæœåŠ¡å™¨ç”¨äºæ”¯æŒå¤§é‡ç”¨æˆ·çš„è®¿é—®ã€‚æœåŠ¡å™¨çš„ç¡¬ä»¶é…ç½®é€šå¸¸æ¯”ä¸ªäººç”µè„‘æ›´é«˜ï¼Œä»¥ä¿è¯ç³»ç»Ÿçš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚"}]}
```

Command to convert jsonl dataset to arrow format is similar to the command in [3.1 Data for Pretraining](#31-data-for-pretraining). In `prepare_sft_dataset.py`, we don't concatenate different data samples.
```
python prepare_sft_dataset.py.py \
    --data_input_dirs "<JSONL_DIR_1>,<JSONL_DIR_2>,<JSONL_DIR_3>" \
    --tokenizer_dir "<TOKENIZER_DIR>" \
    --data_output_dirs "spliced tokenized output" \
    --max_length 4096 \
    --num_spliced_dataset_bins 10 \
    --llama_version 3
```

Additional CLI arguments:
* LLaMA verison: `llama_version`. Specify the LLaMA version.

#### 4. Command Line Arguments for Training

##### 4.1 Arguments for Pretraining
You can use `colossalai run` to launch multi-nodes training:
```bash
colossalai run --nproc_per_node YOUR_GPU_PER_NODE --hostfile YOUR_HOST_FILE \
train.py --OTHER_CONFIGURATIONS
```
Here is a sample hostfile:
```bash
hostname1
hostname2
hostname3
hostname4
```
Make sure master node can access all nodes (including itself) by ssh without password.

Here is details about CLI arguments:
* Pre-trained model path: `--pretrained`. Path to the pre-trained model in Hugging Face format.
* Dataset path: `--dataset`. Path to the pre-tokenized dataset.
* Booster plugin: `--plugin`. `ddp`,`gemini`, `gemini_auto`, `zero2`ï¼Œ`zero2_cpu` and `3d` are supported.For more details, please refer to [Booster plugins](https://colossalai.org/docs/basics/booster_plugins/).
* Intermediate checkpoint to load: `--load_checkpoint`. Path to the intermediate checkpoint. Saved checkpoint contains the states for `lr_scheduler`, `optimizer`,`running_states.json` and `modelling`. If `load_checkpoint` points to the `modelling` folder, only the model weights will be loaded without any other states to support multi-stage training.
* Save interval: `--save_interval`. The interval (steps) of saving checkpoints. The default value is 1000.
* Checkpoint directory: `--save_dir`. The directory path to save checkpoint and intermediate states. Intermediate states include `lr_scheduler`, `optimizer`,`running_states.json` and `modelling`.
* Tensorboard directory: `--tensorboard_dir`. The path to save tensorboard logs.
* Configuration file: `--config_file`. The path to save the configuration file.
* Number of epochs: `--num_epochs`. Number of training epochs. The default value is 1.
* Batch size: `--batch_size`. Batch size per GPU. The default value is 1. For PP, it refers to number of samples per step.
* Learning rate: `--lr`. The default value is 3e-4.
* Max length: `--max_length`. Max context length. The default value is 4096.
* Mixed precision: `--mixed_precision`. The default value is "fp16". "fp16" and "bf16" are supported.
* Gradient clipping: `--gradient_clipping`. The default value is 1.0.
* Weight decay: `--weight_decay`. The default value is 0.1.
* Warmup steps: `--warmup_steps`. The default value is calculated by 0.025 warmup ratio.
* Gradient checkpointing: `--use_grad_checkpoint`. The default value is `False`. This saves memory at the cost of speed. You'd better enable this option when training with a large batch size.
* Flash attention: `--use_flash_attn`. If you want to use flash attention, you must install `flash-attn` and related packages. The default value is `False`. This is helpful to accelerate training while saving memory. We recommend you always use flash attention.
* Freeze non-embedding parameters: `--freeze_non_embeds_params`. Freeze non-embedding parameters. It can be helpful to align embeddings after extending vocabulary size.
* Tensor parallelism size: `--tp`. TP size for 3d parallelism. The default value is 1. Used for 3d plugin.
* Pipeline parallelism size: `--pp`. PP size for 3d parallelism. The default value is 1. Used for 3d plugin.
* Sequence parallelism size: `--sp`. SP size for 3d parallelism. The default value is 1. Used for 3d plugin.
* Zero stage: `--zero`. Zero stage for 3d Parallelism. The default value is 1. Used for 3d plugin.
* Sequence parallelism mode: `--sp_mode`. SP mode, used for 3d plugin. Choose from "split_gather", "ring", "all_to_all".
* Switch for sequence parallelism: `--enable_sequence_parallelism`. Whether to enable SP, used for 3d plugin.
* Zero CPU offload: `--zero_cpu_offload`. Whether to use offloading, used for 3d plugin.
* Micro batch size: `--microbatch_size`. Batch size for each process in PP, used for 3d plugin.
* Number of dummy sample: `--num_samples`. Number of samples for benchmarking.
* Benchmark switch: `--benchmark`. Benchmark performance using random dataset.

##### 4.2 Arguments for Supervised Fine-tuning
We add support for gradient accumulation and NEFTuning for supervised fine-tuning and thus there are two more arguments apart from the arguments listed in [4.1 Arguments for Pretraining](#41-arguments-for-pretraining).

Here is details about CLI arguments:
* Accumulation steps: `--accumulation_steps`. The default value is `8`.
* NEFTuning: `--use_neft`. The default value is `False`. It can help improve the performance of chat models.

#### 5. Running Command

##### 5.1 Command for Pretraining
An [example bash](train.example.sh) is also provided for the experiment. Here is the steps to run the experiment:
* Create your own hostfile: `cp hostfile.example hostfile`.
* Create your own bash: `cp train.example.sh train.sh`.
* Add your real host ip or host name into the `hostfile`.
* Update global variables and parameters in your `train.sh`.
* Run the experiment by `bash train.sh`

Here is the details about global variables for each experiment:
* `PROJECT_NAME`: Project name for each experiment.
* `PARENT_SAVE_DIR`: Parent folder to save model checkpoint.
* `PARENT_TENSORBOARD_DIR`: Parent folder to save tensorboard logs.
* `PARENT_CONFIG_FILE`: Parent folder to save configuration for each experiment.
* `PRETRAINED_MODEL_PATH`: Path to the local pre-trained model checkpoint.
* `dataset`: Paths to all prepared data. Typically, it's a list of subfolders within the output path of prepare data, `--data_arrow_output_dir`, and if there are multiple subfolders, please list them all. e.g.,
```python
declare -a dataset=(
    "<DIR_1>/part-00000"
    "<DIR_1>/part-00001"
    "<DIR_2>/part-00000"
)
```

##### 5.2 Command for Supervised Fine-tuning
An [example bash](train_sft.example.sh) is provided. The only difference with the command for pretraining is the two arguments (`--accumulation_steps` and `--use_neft`) in the script. You can refer to [4.2 Arguments for Supervised Fine-tuning](#42-arguments-for-supervised-fine-tuning) for more details.

## Technical Insights
In order to enhance LLaMA-2's capabilities for understanding and generating Chinese content, The [Colossal-AI](https://github.com/hpcaitech/ColossalAI) team proposes the continuation of pre-training the LLaMA-2 model using both Chinese and English corpora. The overall pipeline can be described as follows:

<p id="Colossal-LLaMA-2-pipeline" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/Colossal-LLaMA-2-pipeline.jpeg?raw=true" width=800/>
</p>

### Data
Large language models such as LLaMA-2 have undergone training using a heterogeneous blend of high-quality datasets, yielding promising outcomes. Enhancing LLaMA-2's performance for the Chinese corpus, while preserving its proficiency in English, critically hinges on two pivotal factors: the composition of the dataset, which encompasses both English and Chinese content, and the quality of each constituent dataset.

The following figure shows the data processing pipeline conducted for Colossal-LLaMA-2.
<p id="Colossal-LLaMA-2-data-processing-pipeline" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/data_processing_pipeline.jpeg?raw=true" width=800/>
</p>

â—ï¸**Important**: We will open-source our data-processing toolkit soon, stay tuned!

### Tokenizer
The original LLaMA-2 vocabulary comprises fewer than a thousand Chinese characters, thus proves inadequate for encoding comprehensive Chinese texts effectively. Secondly, the utilization of byte tokens presents a challenge for transformer encoders to capture the semantic nuances of Chinese characters.

To address the above issues, we extend LLaMA-2 vocabulary from 32,000 to 69,104. To adapt the LLaMA-2 model for use with the Colossal-LLaMA-2 tokenizer, we initialize the new word embeddings by calculating the mean values from the original LLaMA-2 embeddings and subsequently append these new rows to the end of the original embedding matrices.

Advantages of extending vocabulary size:
* Improve the compression rate of string sequence encoding.
* Enhance the integrity of information.
* Enable encoded sequences to contain more valuable information, thereby theoretically enhancing the ability for chapter-level encoding.

Advantages of large vocabulary size under low-resource settings:
* The presence of numerous unused tokens can be attributed to the limited training dataset, where an excessive number of tokens might not have been effectively learned.
* Excessive vocabulary expansion leads to an increase in embedding-related parameters, resulting in higher memory usage, which, in turn, affects the efficiency of the training process.

To balance both sides, we finally construct our vocabulary with size 69,104. The following table below presents a comparison of various models at the 7B level.

| Model | Vocabulary Size | Compression Rate | Average Length of Samples (token-level) |
| :-----------: | :---------: | :----: | :----: |
| Colossal-LLaMA-2 | 69104 | 0.659 | 73.682 |
| LLaMA-2-7B | 32000 | 1.205 | 134.689 |
| Atom-7B | 65000 | 0.634 | 70.915 |
| Baichuan-7B | 64000 | 0.678 | 75.857 |
| Baichuan2-7B-base | 125696 | 0.570 | 63.761 |
| Chatglm2-6B | 64789 | 0.645 | 72.178 |
| InternLM-7B | 103168 | 0.566 | 63.349 |
| Qwen-7B | 151643 | 0.578 | 64.703 |
| Tigerbot-7B-base | 60515 | 0.630 | 70.515 |
| Yayi-7B-llama2 | 32005 | 1.214 | 135.689 |
| Chinese-llama-2-7b | 55296 | 0.668 | 74.690 |
| Chinese-Falcon-7B | 90046 | 0.669 | 74.858 |
| LinkSoul-Chinese-Llama-2-7b | 40076 | 0.958 | 107.089 |
| Ziya-LLaMA-13B-v1.1 | 39410 | 0.958 | 107.074 |


### Training Strategy
#### Multi-stage Training
In order to enhance the model's performance and harness the full potential of the original LLaMA-2, we have developed a multi-stage training strategy. This strategy is designed to systematically unlock the model's capabilities over a series of stages.

Therefore, we have divided the training process into three stages:
* Large-scale pre-training stage (Conducted by LLaMA-2): This initial stage is aimed at establishing the model's foundational capabilities from the ground up. It necessitates the use of a substantial dataset comprising no less than 1 trillion tokens.
* Chinese knowledge injection stage: In this stage, we introduce Chinese knowledge into the model. It requires access to a high-quality dataset rich in comprehensive knowledge relevant to the Chinese language.
* Knowledge replay stage: Knowledge is replayed through a question-answering (QA) mechanism, encompassing both the Chinese and English domains.

Following the completion of this multi-stage training process, the model exhibits notable improvements in performance across both English and Chinese benchmarks.

The following figure illustrates the three stages for training Colossal-LLaMA-2.

<p id="Colossal-LLaMA-2-Multi-stage-training" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/multi-stage-training.png?raw=true" width=600/>
</p>

#### Bucket-based Training
Our experiments have revealed that the distributions within the training dataset, as well as the arrangement of various topic-related data points, significantly impact the overall performance of the model, particularly in the context of continual pre-training of LLaMA-2.

In an effort to achieve a more balanced distribution and exert control over the dataset's ordering, we have adopted a method where we divide each sub-dataset into discrete bins. These bins are then combined to construct individual data buckets, with one bin contributed by each sub-dataset.

### Bridging Any Domain-specific Large Models
Applying the above process to perform knowledge transfer in any field allows for the cost-effective construction of lightweight domain-specific foundational large models.

<p id="domain_specific-llm" align="center">
<img src="https://github.com/hpcaitech/public_assets/blob/main/applications/colossal-llama-2/domain_specific-llm.jpeg?raw=true" width=800/>
</p>

## Citations
```bibtex
@article{bian2021colossal,
    title={Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
    author={Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},
    journal={arXiv preprint arXiv:2110.14883},
    year={2021}
}
```
```bibtex
@misc{touvron2023llama,
    title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
    author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
    year={2023},
    eprint={2307.09288},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```
```bibtex
@article{dao2023flashattention2,
    title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
    author={Dao, Tri},
    year={2023}
}
```
```bibtex
@article{jain2023neftune,
    title={NEFTune: Noisy Embeddings Improve Instruction Finetuning},
    author={Jain, Neel and Chiang, Ping-yeh and Wen, Yuxin and Kirchenbauer, John and Chu, Hong-Min and Somepalli, Gowthami and Bartoldson, Brian R and Kailkhura, Bhavya and Schwarzschild, Avi and Saha, Aniruddha and others},
    journal={arXiv preprint arXiv:2310.05914},
    year={2023}
}
```
